---
layout: post
title:  "Exploratory data analysis on hacker news stories with NLTK"
excerpt: "How to load a json corpus with NLTK and perform a quick analysis on hacker news top stories"
date:   2022-01-23
categories: [EDA, NLP, tokenizer]
tags: [NLTK, json]
---

![Split of an apple](/assets/2022-01-23/split-apple.jpg)

# Explore with NLTK

## Concordance
```python
corpus_metric.story_text.concordance("language")
```

## Frequency distribution
```python
corpus_metric.frequency_distribution.most_common(20)
corpus_metric.frequency_distribution.plot(20, cumulative=True)
```

![alt](/assets/2022-01-23/frequency-distribution.png)

## Lexical dispersion plot

```python
corpus_metric.story_text.dispersion_plot(
    [
        "Google",
        "Microsoft",
        "Apple",
        "Amazon",
        "Tesla",
    ]
)
```

![alt](/assets/2022-01-23/lexical-dispersion-plot.png)

## Word Cloud

![alt](/assets/2022-01-23/word-cloud.png)

## Recurrent pattern

```python
topstories[topstories["title"].str.contains("Ask HN")]["title"]
```

## Collocations with Dunning likelihood method

```python
corpus_metric.story_text.collocations() # Dunning likelihood collocation
# three word window: corpus_metric.story_text.collocations(window_size=3)
```

open source; command line; Dark Souls; Nhat Hanh; Points Guy; Suite
legacy; Thich Nhat; largest chip; American Airlines; black holes; SICP
JavaScript; modern language; Google Analytics; source Ask

```python
topstories[topstories["title"].str.contains(" source")]["title"]
```

# Descriptive metrics


We define a corpus $$\mathcal{C} = {d_i} $$ as a list of document. The number of document can be seen as the cardinality of the corpus $$\vert \mathcal{C} \vert = N$$.

In this case, each document has only one text field, the title.

Each document has a set of text attribute/field. Each field is indexed with j from 1 to M, such $$a_{ij}$$ is the j$$^{th}$$ field of the i$$^{th}$$ document.

We define a tokenizer function $$ f_{tokenizer} $$ as $$ f_{tokenizer} : a_{ij} \rightarrow [t_1, ..., t_k, ... , t_L] $$, with $${t_k}$$ the list of tokens corresponding to the $$ a_{ij} $$ field.

We define $$ \mathcal{T} = t_{ijk} $$ the token k for the field j of the ith document.
We can also note $$ \mathcal{T}_j = t_{ik} $$ the list of token for a given field j.

We define the dictionary $$ \mathcal{D} = { t_i } $$ given a tokenizer function $$ f_{tokenizer} $$, as the list of unique tokens generated by a tokenizer on a given corpus.

| Metric | Formula | Tokenizer sensitive |
|--|--|--|
| Item Count | $$ \vert \mathcal{C} \vert $$ | No |
| Item Unique Count | $$ \vert \mathcal{C}_{unique} \vert $$ | No |
| Duplicate Item Proportion | $$ \vert \mathcal{C} \vert - \vert \mathcal{C}_{unique} \vert \over \vert \mathcal{C} \vert $$ | No |
| Dictionary Length | $$ \vert \mathcal{D} \vert $$ | Yes |
| Lemmatized Dictionary Length | $$ \vert \mathcal{D}_{lemme} \vert $$ | Yes |
| In vocabulary Token Proportion | $$ \vert \mathcal{D}_{lemme} \cap \mathcal{D}_{NLTK} \vert \over \vert \mathcal{D}_{lemme} \vert $$ | Yes |
| Out of vocabulary Token Proportion (wordnet vocabulary) <br> Domain specific metric | $$ \vert \mathcal{D}_{lemme} \vert - \vert \mathcal{D}_{lemme} \cap \mathcal{D}_{NLTK} \vert \over \vert \mathcal{D}_{lemme} \vert $$ | Yes |
| Token Count | $$ \vert \mathcal{T} \vert $$ | Yes |
| Lexical Diversity | $$ \vert \mathcal{D} \vert \over \vert \mathcal{T} \vert $$ | Yes |
| Hapaxes Proportion | $$ \vert \mathcal{D}_{hapax} \vert \over \vert \mathcal{D} \vert $$ | Yes |
| Uppercase Items Proportion | $$ n_{upper} \over n_{unique} $$ | No |
| Numerical Token Proportion | $$ d_{numerical} \over d $$ | Yes |
| Average Item Length | $$ \bar{n} $$ | Yes |
| Standard Deviation Item Length | $$ s_{n} $$ | Yes |
| Median Item Length | $$ \tilde{n} $$ | Yes |
| Minimum/Maximum Item Length | $$ min(n), max(n) $$ | Yes |


**Tokenizer specifications**

| Step | Description |
|--|--|
| Replace html entities | Remove html entities like &gt; or &nbsp; and convert them to their corresponding unicode character. |
| Find word according to regex patterns | |
| Filter (remove) words that are punctuation | remove punctuation from `string.punctuation` **'!"#$%&\'()*+,-./:;<=>?@[\\]^_`{\|}~'** and **——–’‘“”×** |


## Create a simple Corpus Metrics class

```python
"n" for nouns
"v" for verbs
"a" for adjectives
"r" for adverbs
"s" for satellite adjectives
```
# WordCloud

# Sources
* https://igraph.org/python/tutorial/latest/install.html
* https://regex101.com
* https://www.tensorflow.org/text/guide/subwords_tokenizer?hl=en#optional_the_algorithm
* https://www.nltk.org/book_1ed
* https://stackoverflow.com/questions/38179829/how-to-load-a-json-file-with-python-nltk

* See Manning, C.D., Manning, C.D. and Schütze, H., 1999. Foundations of Statistical Natural Language Processing. MIT press, p. 162 https://nlp.stanford.edu/fsnlp/promo/colloc.pdf#page=22