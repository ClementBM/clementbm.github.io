---
layout: post
title:  "Compute simple corpus descriptive metrics"
excerpt: "Get a quick overview of a corpus with some basic descriptive metrics with the help of NLTK"
date:   2022-07-06
categories: [theory]
tags: [NLTK, NLP, tokenizer, metrics]
---

![Multiple fruits](/assets/2022-07-06/pexels-trang-doan-1128678.jpg)

In natural language processing we can work on a wide variety of task, Language Modelling, Question Answering, Machine Translation, Text Classification, Text Generation to name a few. As well as on very different datasets either labeled or not. Once the objective of the project is well and concisely described, one may chose one or multiple metrics to evaluate the objective. Imagine your building a model to translate sentences from French to Japanese. You found a handy dataset of paired sentences and chose BLEU score to evaluate the model performance.

To better understand the biases unintentionally learned by the model having some metric describing the dataset can be handy. 

Apart for a good definition of a dataset and its context, it's good to have a quick descriptive card with a bunch of classic metric.

In NLP, these metrics can give an hint on the way to go in the objective.

If you can't measure it, you can't improve it (William Thomson, Lord Kelvin)

# Descriptive metrics

Analyse a text and understand more about its linguistics features. 

Texts that are lexically diverse use a wide range of vocabulary, avoid repetition, use precise language and tend to use synonyms to express ideas.

When discovering a new textual dataset, having a way to dig into, search for characteristic is a good starting point. Having already seen some basic function of text analysis with NLP.Text, we will now give some descriptive number to the corpus we are discovering.

To start, let's define a quick theorical context, before digging into the descriptive metrics.

We define a corpus $$\mathcal{C} = {d_i} $$ as a list of document. The number of document can be seen as the cardinality of the corpus $$\vert \mathcal{C} \vert = N$$.

Each document has a set of text attribute/field. Each field is indexed with j from 1 to M, such $$a_{ij}$$ is the j$$^{th}$$ field of the i$$^{th}$$ document.

In this case, each document has only one text field, the title.

We define a tokenizer function $$ f_{tokenizer} $$ as $$ f_{tokenizer} : a_{ij} \rightarrow [t_1, ..., t_k, ... , t_L] $$, with $${t_k}$$ the list of tokens corresponding to the $$ a_{ij} $$ field.

We define $$ \mathcal{T} = t_{ijk} $$ the token k for the field j of the i$$^{th}$$ document.
We can also note $$ \mathcal{T}_j = t_{ik} $$ the list of token for a given field j.

We define the dictionary $$ \mathcal{D} = { t_i } $$ given a tokenizer function $$ f_{tokenizer} $$, as the list of unique tokens generated by a tokenizer on a given corpus.

|    | Name                        | Formula                                                                                                                                                  | Value   | Description                                          |
|---:|:----------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------|:--------|:-----------------------------------------------------|
|  0 | Count                       | $$\vert \mathcal{O} \vert$$                                                                                                                                | 500     | Number of title                                      |
|  1 | Unique count                | $$\vert \mathcal{O}_{unique} \vert$$                                                                                                                       | 497     | Number of unique title                               |
|  2 | Token count                 | $$\vert \mathcal{T} \vert$$                                                                                                                                | 2853    | Total number of tokens                               |
|  3 | Dictionary length           | $$\vert \mathcal{D} \vert$$                                                                                                                                | 2195    | Total number of unique tokens                        |
|  4 | Lem dictionary length       | $$\vert \mathcal{D}_{lemme} \vert$$                                                                                                                        | 1817    | Total number of unique lemmetized tokens             |
|  5 | Alpha lem dictionary length | $$\vert \mathcal{D}_{\alpha-lemme} \vert$$                                                                                                                 | 1766    | Total number of unique alpha lemmetized tokens       |
|  6 | Average length              | $$\bar{M_i}$$                                                                                                                                              | 49.69   | Average number of tokens                             |
|  7 | Min and Max length          | $$\{ min(M_i), max(M_i) \}$$                                                                                                                               | (7, 81) | Minimum and maximum number of tokens                 |
|  8 | Median length               | $$\tilde{M_i}$$                                                                                                                                            | 51      | Median number of tokens                              |
|  9 | Std length                  | $$s_{M_i}$$                                                                                                                                                | 19.12   | Standard deviation of the number of tokens           |
| 10 | Duplicate proportion        | $$\vert \mathcal{O} \vert - \vert \mathcal{O}_{unique} \vert \over \vert \mathcal{O} \vert$$                                                               | 0.006   | Proportion of title that appears more than once      |
| 11 | Numerical frequency         | $$\vert \mathcal{T}_{numerical} \vert \over \vert \mathcal{T} \vert$$                                                                                      | 0.0315  | Frequency of numerical tokens                        |
| 12 | Numerical proportion        | $$\vert \mathcal{D}_{numerical} \vert \over \vert \mathcal{D}_{lemme} \vert$$                                                                              | 0.0281  | Proportion of numerical tokens                       |
| 13 | In vocabulary               | $$\vert \mathcal{D}_{\alpha-lemme} \cap \mathcal{D}_{NLTK} \vert \over \vert \mathcal{D}_{\alpha-lemme} \vert$$                                            | 0.7599  | Proportion of tokens inside the NLTK vocabulary      |
| 14 | Out of vocabulary           | $$\vert \mathcal{D}_{\alpha-lemme} \vert - \vert \mathcal{D}_{\alpha-lemme} \cap \mathcal{D}_{NLTK} \vert \over \vert \mathcal{D}_{\alpha-lemme} \vert$$ | 0.2401  | Proportion of tokens outside the NLTK vocabulary     |
| 15 | Lexical diversity           | $$\vert \mathcal{D} \vert \over \vert \mathcal{T} \vert$$                                                                                                  | 0.7694  | Dictionary count over the token count                |
| 16 | Hapaxes                     | $$\vert \mathcal{D}_{hapax} \vert \over \vert \mathcal{D} \vert$$                                                                                          | 0.8378  | Proportion of token that occur once (hapax legomena) |
| 17 | Uppercase items             | $$\vert \mathcal{O}_{upper} \vert \over \vert \mathcal{O} \vert$$                                                                                          | 0.0     | Proportion of uppercased title                       |
| 18 | Uppercased token proportion | $$\vert \mathcal{T}_{upper} \vert \over \vert \mathcal{T} \vert$$                                                                                      | 0.0648  | Proportion of uppercased token                       |

To find characteristic and informative thing about the textual dataset, some indicators are good to describe.

Let's return to our task of finding words that characterize a text. Have we succeeded in automatically extracting words that typify a text? Well, these very long words are often hapaxes (i.e., unique) and perhaps it would be better to find frequently occurring long words. This seems promising since it eliminates frequent short words (e.g., the) and infrequent long words (e.g. antiphilosophists). Here are all words from the chat corpus that are longer than seven characters, that occur more than seven times...

## Lexical diversity and Hapaxes Legomena
Not enough context? Characteristic of a small dataset

If the frequent words don't help us, how about the words that occur once only, the so-called hapaxes? View them by typing fdist1.hapaxes(). It seems that there are too many rare words, and without seeing the context we probably can't guess what half of the hapaxes mean in any case! Since neither frequent nor infrequent words help, we need to try something else.

## Uppercased tokens
Hint to abbreviation

## In and Out tokens
Specialization 

**Tokenizer specifications**

| Step | Description |
|--|--|
| Replace html entities | Remove html entities like &gt; or &nbsp; and convert them to their corresponding unicode character. |
| Find word according to regex patterns | |
| Filter (remove) words that are punctuation | remove punctuation from `string.punctuation` **'!"#$$%&\'()*+,-./:;<=>?@[\\]^_`{\|}~'** and **——–’‘“”×** |

## Create a simple Corpus Metrics class

```python
"n" for nouns
"v" for verbs
"a" for adjectives
"r" for adverbs
"s" for satellite adjectives
```

# Sources
* https://www.nltk.org/book/ch01.html


